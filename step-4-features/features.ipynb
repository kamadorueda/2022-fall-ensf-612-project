{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi!\n",
    "\n",
    "This notebook was created using a local Apache Spark cluster.\n",
    "\n",
    "If you are curious on what the deployment looks like,\n",
    "see [this](https://github.com/kamadorueda/machine/blob/main/nixos-modules/spark/default.nix).\n",
    "\n",
    "The cell below is the only thing that needs to be changed if you want to run it,\n",
    "either on DataBricks or TALC.\n",
    "\n",
    "Thank you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "# Step 1: Please change `sc` and `spark`\n",
    "# so that they point to your spark cluster (TALC, DataBricks, etc)\n",
    "\n",
    "conf = pyspark.SparkConf().setMaster(\"local[4]\")\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "spark = pyspark.sql.SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Step 2: Please set the path to the files\n",
    "ISSUES_DIR = \"../step-1-structured-data/issues\"\n",
    "LABELS = \"../step-2-labeling/labels-filled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'issue-number': '18544', 'was-fixed': 'yes'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(LABELS) as file:\n",
    "    labels = tuple(csv.DictReader(file))\n",
    "\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/kamadorueda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kamadorueda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import (\n",
    "    filterfalse,\n",
    ")\n",
    "from nltk.stem import (\n",
    "    PorterStemmer,\n",
    ")\n",
    "import regex\n",
    "import string\n",
    "\n",
    "STEMMER = PorterStemmer()\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "STOP_WORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "ALPHABETIC_AND_SPACE = set(string.ascii_letters + string.whitespace)\n",
    "SPACE = regex.compile(\"\\s+\")\n",
    "WORD_COUNTS = dict()\n",
    "\n",
    "def normalize(text: str) -> list[str]:\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    words = map(STEMMER.stem, words)\n",
    "    words = filterfalse(STOP_WORDS.__contains__, words)\n",
    "    text = \" \".join(words)\n",
    "    text = \"\".join(filterfalse(PUNCTUATION.__contains__, text))\n",
    "    text = \"\".join(filter((ALPHABETIC_AND_SPACE).__contains__, text))\n",
    "    text = SPACE.sub(\" \", text).strip()\n",
    "    words = text.split(\" \")\n",
    "    for word in words:\n",
    "        WORD_COUNTS.setdefault(word, 0)\n",
    "        WORD_COUNTS[word]+=1\n",
    "        \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------+\n",
      "|             content|issue_number|was_fixed|\n",
      "+--------------------+------------+---------+\n",
      "|[powershel, ssh, ...|       18544|     true|\n",
      "|[handl, quotat, m...|       18540|     true|\n",
      "|[problem, psnativ...|       18501|    false|\n",
      "|[question, downlo...|       18500|     true|\n",
      "|[defin, env, var,...|       18497|     true|\n",
      "|[writeerror, pref...|       18490|    false|\n",
      "|[semanticvers, do...|       18489|     true|\n",
      "|[errorresponseexc...|       18485|     true|\n",
      "|[pleas, remov, re...|       18478|     true|\n",
      "|[class, regist, e...|       18476|     true|\n",
      "|[gethelp, paramet...|       18463|     true|\n",
      "|[invokepowershel,...|       18460|     true|\n",
      "|[sendmailmessag, ...|       18459|    false|\n",
      "|[switchprocess, b...|       18436|     true|\n",
      "|[switchprocess, u...|       18433|     true|\n",
      "|[switchprocess, e...|       18432|     true|\n",
      "|[disablewindowsop...|       18431|     true|\n",
      "|[systemprivateuri...|       18424|     true|\n",
      "|[windowstyl, hidd...|       18423|     true|\n",
      "|[releas, process,...|       18422|     true|\n",
      "+--------------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# issue_numbers\n",
    "\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "data = []\n",
    "\n",
    "for label in labels:\n",
    "    issue_number = label[\"issue-number\"]\n",
    "    assert label[\"was-fixed\"] in (\"yes\", \"no\")\n",
    "\n",
    "    with open(os.path.join(ISSUES_DIR, f\"{issue_number}.json\")) as file:\n",
    "        issue = json.load(file)\n",
    "\n",
    "    assert issue[\"title\"] is not None, issue_number\n",
    "\n",
    "    data.append(\n",
    "        {\n",
    "            \"issue_number\": issue_number,\n",
    "            \"content\": normalize(issue[\"title\"] or \"\") + normalize(issue[\"body\"] or \"\"),\n",
    "            \"was_fixed\": label[\"was-fixed\"] == \"yes\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "data = spark.createDataFrame(data)\n",
    "\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"content.csv\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(\n",
    "        file, fieldnames=[\"issue_number\", \"was_fixed\", \"content\"]\n",
    "    )\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in data.collect():\n",
    "        writer.writerow(\n",
    "            {\n",
    "                \"issue_number\": row.issue_number,\n",
    "                \"was_fixed\": row.was_fixed,\n",
    "                \"content\": \" \".join(row.content),\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 17:52:09 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "+------------+--------------------+---------+\n",
      "|issue_number|            features|was_fixed|\n",
      "+------------+--------------------+---------+\n",
      "|       18544|(262144,[5940,829...|     true|\n",
      "|       18540|(262144,[2470,124...|     true|\n",
      "|       18501|(262144,[13828,18...|    false|\n",
      "|       18500|(262144,[5078,853...|     true|\n",
      "|       18497|(262144,[2966,392...|     true|\n",
      "|       18490|(262144,[8297,226...|    false|\n",
      "|       18489|(262144,[15769,19...|     true|\n",
      "|       18485|(262144,[892,7777...|     true|\n",
      "|       18478|(262144,[1206,243...|     true|\n",
      "|       18476|(262144,[4274,594...|     true|\n",
      "|       18463|(262144,[1968,338...|     true|\n",
      "|       18460|(262144,[921,4214...|     true|\n",
      "|       18459|(262144,[1206,433...|    false|\n",
      "|       18436|(262144,[8297,119...|     true|\n",
      "|       18433|(262144,[2139,777...|     true|\n",
      "|       18432|(262144,[2139,777...|     true|\n",
      "|       18431|(262144,[1182,680...|     true|\n",
      "|       18424|(262144,[2139,143...|     true|\n",
      "|       18423|(262144,[1206,137...|     true|\n",
      "|       18422|(262144,[726,892,...|     true|\n",
      "+------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    HashingTF,\n",
    "    IDF,\n",
    ")\n",
    "\n",
    "tf = HashingTF(inputCol=\"content\", outputCol=\"temp\")\n",
    "idf = IDF(inputCol=\"temp\", outputCol=\"features\")\n",
    "\n",
    "data = tf.transform(data)\n",
    "idf_model = idf.fit(data)\n",
    "data = idf_model.transform(data)\n",
    "data = data.select(\"issue_number\", \"features\", \"was_fixed\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 17:52:09 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    }
   ],
   "source": [
    "rows = data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords: 13337\n",
      "taken keywords: 1029\n"
     ]
    }
   ],
   "source": [
    "print(\"keywords:\", len(WORD_COUNTS))\n",
    "# take approximately the top 1000 keywords\n",
    "threshold = sorted(WORD_COUNTS.values(), reverse=True)[999]\n",
    "\n",
    "WORDS = sorted(\n",
    "    [word for word in WORD_COUNTS if WORD_COUNTS[word] >= threshold],\n",
    "    key=lambda word: WORD_COUNTS[word],\n",
    "    reverse=True\n",
    ")\n",
    "WORD_INDEXES = list(map(tf.indexOf, WORDS))\n",
    "print(\"taken keywords:\", len(WORDS))\n",
    "\n",
    "with open(\"features.csv\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(\n",
    "        file, fieldnames=[\"issue_number\", \"was_fixed\", *WORDS]\n",
    "    )\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in rows:\n",
    "        writer.writerow(\n",
    "            {\n",
    "                \"issue_number\": row.issue_number,\n",
    "                \"was_fixed\": row.was_fixed,\n",
    "                **{\n",
    "                    word: row.features[index]\n",
    "                    for word, index in zip(WORDS, WORD_INDEXES)\n",
    "                },\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5f600bb0a23048005804b0ea244c436da6b0532e9328a8119f62e33d9ec479f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
