{
  "_url": "https://github.com/PowerShell/PowerShell/issues/8270",
  "author": "mklement0",
  "body": "`Get-Content -ReadCount` allows you to \"chunk\" the input lines into sub-arrays with a fixed size; e.g.:\r\n\r\n```powershell\r\nPS> 1..5 > t.txt; Get-Content t.txt -ReadCount 2 | % { \"$_\" }\r\n1 2\r\n3 4\r\n5\r\n```\r\n\r\nThat is, the 5 lines were sent as three 2-element arrays through the pipeline (the last array has only 1 element, but is still an array).\r\n\r\nIt would be handy to have the same ability more generally in `Select-Object`, for chunking (partitioning) arbitrary input collections:\r\n\r\n```powershell\r\n# Wishful thinking.\r\n1..4 | Select-Object -ReadCount 2 | % { \"$_\" } # return three 2-element arrays\r\n1 2\r\n3 4\r\n5\r\n```\r\n\r\nPossible combinations / mutual exclusion with existing parameters would have to be fleshed out.\r\n\r\nEnvironment data\r\n----------------\r\n\r\nWritten as of:\r\n\r\n```powershell\r\nPowerShell Core 6.2.0-preview.1\r\n```\r\n",
  "closed_at": null,
  "comments": [
    {
      "author": "BrucePay",
      "author_association": "COLLABORATOR",
      "body": "What scenario did you have in mind for this? The one that has been discussed in the past (#7565) involved parallel computations. However, in that scenario, performance is a concern and I'm not sure a solution based  on `Select-Object` is ideal.",
      "created_at": "2018-11-26T01:19:02Z",
      "updated_at": "2018-11-26T01:19:02Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "Partitioning for the sake of _performance improvements_ via parallel processing of the partitions is definitely one important use of partitioning, but in this case I was thinking of partitioning for _business-logic_ reasons, such as, say, partitioning an array of players into teams of fixed sizes.\r\n\r\nHere's are related (non-performance-related) SO questions:\r\n* https://stackoverflow.com/q/53297509\r\n* https://stackoverflow.com/q/45948580\r\n* https://stackoverflow.com/q/1396048 (C#-specific)\r\n",
      "created_at": "2018-11-26T19:16:22Z",
      "updated_at": "2018-11-26T19:16:22Z"
    },
    {
      "author": "BrucePay",
      "author_association": "COLLABORATOR",
      "body": "@mklement0 Thanks!",
      "created_at": "2018-11-27T23:12:38Z",
      "updated_at": "2018-11-27T23:12:38Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "## *\"Burst Mode\"*\r\nAs far as I can tell batching (not even implying parallel processing), by means of choking the pipeline for a while and than burst the load into batches could boost some of the succeeding cmdlets as e.g. the `Set/Add-Content` - and the `Export-Csv` cmdlet:\r\n\r\n<details>\r\n  <summary>Create-Batch</summary>\r\n\r\nsee also: [Slice a PowerShell array into groups of smaller arrays](https://stackoverflow.com/a/73687067/1701026)\r\n  \r\n```PowerShell\r\nfunction Create-Batch {\r\n    [CmdletBinding()]\r\n    param (\r\n        [ULong]$Size,\r\n        [Parameter(Mandatory=$true, ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true)]$InputObject\r\n    )\r\n    begin {\r\n        $Batch = [Collections.Generic.List[object]]::new()\r\n    }\r\n    process {\r\n        if ($Size -and $Batch.get_Count() -ge $Size) {\r\n            ,$Batch\r\n            $Batch = [Collections.Generic.List[object]]::new()\r\n        }\r\n        $Batch.Add($_)\r\n    }\r\n    End {\r\n        if ($Batch.get_Count()) { ,$Batch }\r\n    }\r\n}\r\n```\r\n</details>\r\n\r\n ```PowerShell\r\n# This takes about 11 seconds:\r\n1..1000000 |Set-Content .\\test1\r\n# This takes about 5 seconds (even there is an addition cmdlet in the chain):\r\n1..1000000 |Create-Batch -Size 10000 |Set-Content .\\test2\r\n# (All in once) takes less than 0.5 seconds:\r\n(,@(1..1000000)) |Set-Content .\\test3\r\n```\r\n\r\n<Strike>\r\n\r\n```PowerShell\r\n# This takes more than 4 seconds\r\n1..100000 |ForEach-Object { [pscustomobject]@{ id = $_; name = \"name$_\" } } |Export-Csv .\\test1.csv\r\n# This takes less than 3 secondss\r\n1..100000 |ForEach-Object { [pscustomobject]@{ id = $_; name = \"name$_\" } } |Create-Batch -Size 10000 |Export-Csv .\\test2.csv\r\n# This takes less than 0.5 seconds:\r\n(,@(1..100000)) |ForEach-Object { [pscustomobject]@{ id = $_; name = \"name$_\"} } |Export-Csv .\\test3.csv \r\n```\r\n</strike>\r\n\r\n(Related: #4242 [Consistently document a scalar `-InputObject` parameter as an implementation detail or make item-by-item processing cmdlets explicitly iterate over collections](https://github.com/PowerShell/PowerShell/issues/4242))\r\n",
      "created_at": "2022-09-12T09:23:50Z",
      "updated_at": "2022-09-12T12:35:10Z"
    },
    {
      "author": "jhoneill",
      "author_association": "NONE",
      "body": "@iRon7   you do realise that last example  creates a 1 row CSV file ?, and the the create-batch example doesn't output the right csv. ?   (It's not 100000  rows of id/name, it  100000 / batch size rows with properties of the list object as columns ? \r\n\r\n\r\n\r\n \r\n",
      "created_at": "2022-09-12T09:57:59Z",
      "updated_at": "2022-09-12T09:57:59Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": ">  and the the create-batch example doesn't output the right csv\r\n\r\nOops, I didn't check the actual `csv` file as just the (display) output looks correct:\r\n```PowerShell\r\n1..5 |ForEach-Object { [pscustomobject]@{ id = $_; name = \"name$_\" } } |Create-Batch -Size 3\r\nid name\r\n-- ----\r\n 1 name1\r\n 2 name2\r\n 3 name3\r\n 4 name4\r\n 5 name5\r\n\r\n(1..5 |ForEach-Object { [pscustomobject]@{ id = $_; name = \"name$_\" } } |Create-Batch -Size 3).Count\r\n2\r\n```\r\nWhich seems about right, but the `Export-Csv` cmdlet apparently doesn't accept \"batches\" (I don't see any reason why not).\r\n\r\nAnyways,\r\nI have removed the `Export-Csv` part  from my [earlier comment](https://github.com/PowerShell/PowerShell/issues/8270#issuecomment-1243457103) but that still leaves the `Set/Add-Content` cmdlet",
      "created_at": "2022-09-12T11:53:48Z",
      "updated_at": "2022-09-12T12:36:02Z"
    },
    {
      "author": "jhoneill",
      "author_association": "NONE",
      "body": "> Which seems about right, but the `Export-Csv` cmdlet apparently doesn't accept \"batches\" (I don't see any reason why not).\r\n\r\n  `Set-Content` / `Add-Content`  seem to know they should expand  `[Collections.Generic.List[object]]`  be `Export-csv` seems not to so exports the list's properties, not the members of the list.  It might be worth opening an issue for that. \r\n\r\nThe original point that `set-content` is quicker with batches, does hold. \r\nIf `Set-Content` gathers content and sends it to the file at the end, adding an X-element array to the buffered items is likely to be quicker than X single adds, although when buffered data is written the file operations will be the same either way - I haven't pulled the code apart to find out if that's how it _does_ work\r\n\r\nTesting these three \r\n```\r\n1..500kb | Set-Content .\\foobar.txt\r\n1..500kb | Create-Batch -Size 50kb | Set-Content .\\foobar.txt \r\n$x = 1..500KB ;  Set-Content .\\foobar.txt -Value $x\r\n```\r\nThe first took 9.6 secs, the second 4.0 , the last 0.25.  (Not using a variable shaves a few more hundredths off) \r\n\r\nSo the best way might be to make one big batch ",
      "created_at": "2022-09-12T13:22:52Z",
      "updated_at": "2022-09-12T13:22:52Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "@iRon7,I think this ultimately comes down to the issue that you've already referenced in a previous comment:\r\n* https://github.com/PowerShell/PowerShell/issues/4242\r\n\r\nIn short, the speedup comes from passing all output as a  _single_ object, and therefore calling `ProcessRecord()` on the downstream cmdlet only _once_, rather than once for _each element_ of an _enumerated_ collection.\r\n\r\nIt is solely an implementation detail of `Set-Content` and `Out-File` that they perform enumeration _themselves_ when given a collection as a whole - most cmdlets, such as `Export-Csv` do _not_ do that.\r\n\r\nIt is that implementation detail that also allows you to simply - and efficiently - pass a collection as a whole to the `-InputObject` parameter (`-Value` in the case of `Set-Content`) _as an argument_; again, this doesn't work meaningfully for most cmdlets, as discussed in the linked issue.\r\n\r\nThus, the much faster alternatives to `1..1e5 | Set-Content tmp.txt` and `1..1e5 | Out-Content tmp.txt`  are\r\n `Set-Content tmp.txt -Value (1..1e5)` and `Out-File tmp.txt -InputObject (1..1e5)` \r\n or, via the pipeline, using an aux. array to send the array as a whole:\r\n `, (1..1e5) | Set-Content tmp.txt` and `, (1..1e5) | Out-File tmp.txt`\r\n\r\n(As an aside: `Out-File` is much slower compared to `Set-Content`; while only the former uses the formatting system, which adds overhead, I'm still surprised at how large the difference is, given that integers are ultimately also just stringified by the formatting system.)\r\n\r\n\r\n\r\n\r\n\r\n",
      "created_at": "2022-09-12T14:11:32Z",
      "updated_at": "2022-09-12T14:11:32Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "@mklement0, thanks for the comments\r\n\r\n> It is that implementation detail that also allows you to simply - and efficiently - pass a collection as a whole to the -InputObject parameter (-Value in the case of Set-Content) as an argument; again, this doesn't work meaningfully for most cmdlets, as discussed in the linked issue.\r\n\r\nAgree, but the reason I started commenting in here, is that \"batch processing\" might give you the possibility to balance between performance and memory. Similar to the `-ReadCount` parameter as apposed to the `-Raw` parameter.\r\n\r\n> ... most cmdlets, such as Export-Csv do not do that\r\n\r\nBut probably *the most common* `Write-Output` cmdlet (default output) does, which actually sets the user's expectation.",
      "created_at": "2022-09-12T14:33:05Z",
      "updated_at": "2022-09-12T15:15:19Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "Understood, @iRon7, but I fear that a solution is only feasible by:\r\n\r\n* (a) updating _individual cmdlets_ to make them do their own enumeration of collection-as-a-whole input\r\n\r\n* (b) clearly documenting this support, so users become aware of it, and noting that it's up to _each cmdlet_ whether to support it or not; this would also serve to make users aware that they need extra effort in order to implement such support in their own cmdlets.\r\n\r\nIn \r\n\r\n* https://github.com/PowerShell/PowerShell/issues/18071\r\n\r\nyou've suggested updating `ConvertTo-Csv` and `Export-Csv`, specifically, but it's worth compiling a comprehensive list of standard cmdlets that could benefit.\r\n\r\n\r\n\r\n",
      "created_at": "2022-09-12T18:07:15Z",
      "updated_at": "2022-09-12T18:07:48Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "To summarize:\r\n\r\n* The chunking mechanism suggested in this issue is motivated by _business logic_, not performance.\r\n\r\n* However, with downstream cmdlets that support performing their own enumeration of collections received as a whole, it could be used for performance optimization too (as you state).\r\n\r\n",
      "created_at": "2022-09-12T18:14:15Z",
      "updated_at": "2022-09-12T18:14:57Z"
    },
    {
      "author": "dkaszews",
      "author_association": "CONTRIBUTOR",
      "body": "Thinking out loud: what about allowing the chunking parameter to be a scriptblock taking context instead of a fix number? For example, it would allow something like this:\r\n\r\n```powershell\r\n@'\r\nSome command output\r\nThat is separated \r\nBy empty lines\r\n\r\nAfter empty line\r\nThis is a new chunk\r\n\r\nChunks may be of different length\r\n'@ | Select-Object -ReadCount { $_.Length -ne 0 } | %{ $_ -join '' } \r\n```",
      "created_at": "2022-09-12T19:18:28Z",
      "updated_at": "2022-09-12T19:20:37Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "That's an interesting idea, @dkaszews, but I'm unclear on the precise logic:\r\n\r\nWhat happens once the first empty line is hit (apart from emitting the previous batch)?\r\nIs the idea that you would _skip_ those input objects that don't fulfill the criterion?\r\nIn other words: is the criterion meant to act as a _separator_ of sorts?\r\n\r\n\r\n",
      "created_at": "2022-09-12T21:09:37Z",
      "updated_at": "2022-09-12T21:09:37Z"
    },
    {
      "author": "dkaszews",
      "author_association": "CONTRIBUTOR",
      "body": "@mklement0 I don't think skipping over the matching line is a good idea, as in some cases it may contain useful data. Think about matching either an opening or closing XML tag, those lines should be added either to current or new batch. For most control, you could return an enum with values like:\r\n\r\n* `'Continue'` - add item to current batch\r\n* `'Skip'` - discard item, start new batch\r\n* `'First'` - start new batch, then add item to it\r\n* `'Last'` - add item to current batch, then start new one\r\n\r\nwith true/false translating to some of them, first two sound reasonable to replicate behavior of splitting string. ",
      "created_at": "2022-09-13T05:09:14Z",
      "updated_at": "2022-09-13T05:09:14Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "Whatever is designed here, keep in mind that *defining* batches would probably mean *redefining* batches:\r\n\r\n```PowerShell\r\nGet-Content .\\t.txt -ReadCount 3 |Select-Object -ReadCount 4 |...\r\n```\r\n\r\nor:\r\n```PowerShell\r\n... |Select-Object -ReadCount 3 |Select-Object -ReadCount 4 |...\r\n```\r\n\r\nMeaning that `Select-Object -ReadCount` should enroll any existing batches and recreate them (rather than embedding the existing batches -with 3 child items- into parent batches)\r\n",
      "created_at": "2022-09-13T07:33:08Z",
      "updated_at": "2022-09-13T07:33:08Z"
    },
    {
      "author": "dkaszews",
      "author_association": "CONTRIBUTOR",
      "body": "@iRon7 Can you clarify what you mean? You're bringing up an important point, I'm just not sure I get what you think valid output is. I would expect the chunks to compose:\r\n\r\n```pwsh\r\n0..9 | Select-Object -ReadCount 2 | Should -Be (ConvertFrom-Json @'\r\n[\r\n  [ 0, 1 ], \r\n  [ 2, 3 ], \r\n  [ 4, 5 ], \r\n  [ 6, 7 ], \r\n  [ 8, 9 ]\r\n] \r\n'@)\r\n\r\n0..9 | Select-Object -ReadCount 2 | Select-Object - ReadCount 3 | Should -Be (ConvertFrom-Json @'\r\n[\r\n  [\r\n    [ 0, 1 ], \r\n    [ 2, 3 ], \r\n    [ 4, 5 ]\r\n  ], \r\n  [\r\n    [ 6, 7 ], \r\n    [ 8, 9 ]\r\n  ] \r\n] \r\n'@)\r\n```\r\n\r\nWhat we are calling \"batches\" are simply arrays of objects, which are later treated as single object each but subsequent cmdlets. If you want the \"chunk\" to be unrolled, you should pipe it explicitly through an unrolling cmdlet, I think something like `%{ $_ | %{ Write-Output $_ } }` should work.\r\n\r\nIt would also make sense to add both at the same time, as they complement each other. If it does not exist already, I think `-Flatten` would make sense, as it is common name in C# LINQ extensions for `.SelectMany(x => x)`. ",
      "created_at": "2022-09-13T07:49:56Z",
      "updated_at": "2022-09-13T07:49:56Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "What I mean is:\r\n```PowerShell\r\n0..9 | Select-Object -ReadCount 2 | Select-Object -ReadCount 3 | Should -Be (ConvertFrom-Json @'\r\n[\r\n  [ 0, 1, 2 ], \r\n  [ 3, 4, 5 ], \r\n  [ 6, 7, 8 ], \r\n  [ 9 ]\r\n] \r\n'@)\r\n```\r\n\r\nI think putting chucks into chucks is quiet confusing and doesn't provide much meaning therefore I also actually prefer the word \"batch\".\r\n\r\n> It would also make sense to add both at the same time, as they complement each other.\r\n\r\nMight indeed also be an option...",
      "created_at": "2022-09-13T08:11:44Z",
      "updated_at": "2022-09-13T08:14:12Z"
    },
    {
      "author": "dkaszews",
      "author_association": "CONTRIBUTOR",
      "body": "@iRon7 What is the exact type returned by `Get-Content $file -ReadCount 3`? If it's an array, then you can't really implement what you suggest without breaking cmdlets that yield arrays, as it is not possible to distinguish them from chunks/batches. While things like arrays of arrays might be confusing to some, I think it's the only consistent way to do things without adding sharp corner cases, similar to existing cmdlets like `Get-ChildItem` unwrapping single results.\r\n\r\nI also think that \"arrays of arrays\" being confusing is caused by how PowerShell displays objects by default, hiding the containers, so that you can't e.g. distinguish a dict from a `PSCustomObject` from an array of key-value pairs. That's why we often use `ConvertTo/From-Json` when discussing containers, to show them exactly. I wonder if adding `Format-Debug` similar to Python's `repr` would help. ",
      "created_at": "2022-09-13T08:19:43Z",
      "updated_at": "2022-09-13T08:19:43Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "> you can't really implement what you suggest\r\n\r\nI can't really see why not?\r\n(Specifically for this feature,) you might simply flatten any array one level:\r\n\r\n<details>\r\n  <summary>Create-Batch (version 2)</summary>\r\n\r\nsee also: [Slice a PowerShell array into groups of smaller arrays](https://stackoverflow.com/a/73687067/1701026)\r\n  \r\n```PowerShell\r\nfunction Create-Batch {\r\n    [CmdletBinding()]\r\n    param (\r\n        [ULong]$Size,\r\n        [Parameter(Mandatory=$true, ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true)]$InputObject\r\n    )\r\n    begin {\r\n        $Batch = [Collections.Generic.List[object]]::new()\r\n    }\r\n    process {\r\n        ForEach ($Item in $_) {\r\n            if ($Size -and $Batch.get_Count() -ge $Size) {\r\n                ,$Batch\r\n                $Batch = [Collections.Generic.List[object]]::new()\r\n            }\r\n            $Batch.Add($Item)\r\n        }\r\n    }\r\n    End {\r\n        if ($Batch.get_Count()) { ,$Batch }\r\n    }\r\n}\r\n```\r\n</details>\r\n\r\n```PowerShell\r\n0..9 |Create-Batch -Size 2 |ConvertTo-Json -Compress\r\n[[0,1],[2,3],[4,5],[6,7],[8,9]]\r\n```\r\n```PowerShell\r\n0..9 |Create-Batch -Size 2 |Create-Batch -Size 3 |ConvertTo-Json -Compress\r\n[[0,1,2],[3,4,5],[6,7,8],[9]]\r\n```\r\n(but is still something to consider)\r\n\r\n> I wonder if adding Format-Debug similar to Python's `repr` would help\r\n\r\nI am not very known with Python, but apparently that is similar to my request: [`#17701` PowerShell should be better able to reveal developer information on an unknown object](https://github.com/PowerShell/PowerShell/issues/17701).",
      "created_at": "2022-09-13T10:08:36Z",
      "updated_at": "2022-09-14T11:03:37Z"
    },
    {
      "author": "jhoneill",
      "author_association": "NONE",
      "body": "> What I mean is:\r\n> \r\n> ```powershell\r\n> 0..9 | Select-Object -ReadCount 2 | Select-Object -ReadCount 3 | Should -Be (ConvertFrom-Json @'\r\n> [\r\n>   [ 0, 1, 2 ], \r\n>   [ 3, 4, 5 ], \r\n>   [ 6, 7, 8 ], \r\n>   [ 9 ]\r\n> ] \r\n> '@)\r\n> ```\r\n> \r\n\r\nThis is changing the behaviour of Select object\r\n```\r\n>   set-content 1,2,3,4,5,6,7  -Path foo.txt ;\r\n>  Get-Content .\\foo.txt -ReadCount 2 | select -Index 2        \r\n5\r\n6\r\n```\r\nIf you pass array objects to `Select-Object` they are **not** unrolled and if they were, there would be no option to get the Nth array. \r\nChanging this is probably too big a break to be accepted.  \r\n\r\nAlthough idea at the start makes sense (whatever term one uses for the arrays that result), and can help some performance issues and some business logic, **I'd question whether it belongs in `Select-Object` .**    \r\nIf someone combines `-ReadCount` and `-Unique`, do we test the array combinations for uniqueness (and does [A , B] equal [B , A]) ?   \r\nCombined with `-First` / `-Last` and/or `Skip` do we group the first/last n objects, or output the first /last n arrays ?    \r\nHow often would we want to combine  `-ReadCount` with `-Property` etc.  (I've seen criticism of `Select-Object` that it does both select rows and select columns, adding \"bunch objects into arrays\" isn't selection)\r\n\r\nI also think the places where it is useful are quite limited. For `Set-Content` it offers a performance gain  so something like \r\n`Get-User | Select-object -Property Name -ReadCount 100 | Set-Content -File names`    \r\nWould mask a failing in `Set-Content`. But the real answer is to fix that (or for _any_ logic to include batching where that makes sense.).\r\nTBH \"Get-user\" is going to take most of the time that example takes to run, so even if we reduce the `Set-Content` time to zero we won't see orders of magnitude improvement    \r\nFor few users (e.g. if the command is really \"Get-LocalUser\") it doesn't matter.    \r\nFor many users, I can see the decision going from \"Can I send as one batch to help perf that I can see is/will be poor?\"  to a long process of what the size should be - if the command is really \"Get-AdUser\" and the directory is 128K users, do I divide into 2  64K chunks? or 64 2K chunks? Or 2K chunks of 64 each ?   And when the next person looks at the code they need to find out why chunks of 512 were chosen. Extra time writing and maintaining the code cancels out the saved run time  :-(    \r\n(see https://jhoneill.github.io/powershell/2019/11/10/Arrays-DontSweatSmallStuff.html) \r\n\r\n  \r\n@iRon7  it may be best to make this available as a community command.  (I'm on the cmdlet working group, and I can't speak for everyone else, but I'd predict this would get a response of \"Good idea, better done by the community\").  Despite the limits of where it is useful, where it does have value it probably has _a lot_ of value. \r\nNote that in V2 you've broken this form `Create-Batch -Size 10  (1..60)`   - you can also have \"a -NoUnroll\" option to batch to arrays of arrays. \r\n\r\n\n\n<blockquote><img src=\"/assets/james.jpg\" width=\"48\" align=\"right\"><div><strong><a href=\"https://jhoneill.github.io/powershell/2019/11/10/Arrays-DontSweatSmallStuff.html\">PowerShell Arrays, performance and [not] sweating the small stuff.</a></strong></div><div>PowerShell (or PWSH if you prefer). Devops (especially Azure Devops), Photography, and general thoughts</div></blockquote>",
      "created_at": "2022-09-13T13:30:05Z",
      "updated_at": "2022-09-13T13:30:12Z"
    },
    {
      "author": "dkaszews",
      "author_association": "CONTRIBUTOR",
      "body": "@jhoneill Putting it into a separate command makes sense, otherwise we're running risk of some unintuitive behavior when what we are selecting is also a collection. Letting community develop it as a separate module, maybe to pull it later in the future does make sense, as making it a new cmdlet negates any need to put it into the core repo, maybe save for some code reuse.\r\n\r\n@iRon7 Is the code you pasted above written for sake of the example, or do you have a public repo we could use for developing such a module? ",
      "created_at": "2022-09-13T13:47:09Z",
      "updated_at": "2022-09-13T13:47:09Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "I also agree that putting it in separate command makes sense.\r\n\r\n> Is the code you pasted above written for sake of the example, or do you have a public repo\r\n\r\nYes and yes, I have quickly setup a new repo: https://github.com/iRon7/Create-Batch/\r\nAt the moment I am too busy to add more to it but plan to do it later.\r\nThe contribution from my side can only be PowerShell based script or module (as I lack the C# knowledge), the initial idea, testing and some basic descriptions (as I am not native English).",
      "created_at": "2022-09-13T14:09:54Z",
      "updated_at": "2022-09-13T14:09:54Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "* @dkaszews, I like [your suggestion](https://github.com/PowerShell/PowerShell/issues/8270#issuecomment-1244905016): it amounts to a very flexible batching mechanism.\r\n\r\n* @iRon7, I also don't think that a batching command should _re_-batch: it should be up to any upstream cmdlet to stream the objects to be batched as-is.\r\n\r\n* As for where the functionality belongs / naming:\r\n  * _Conceptually_, I think it makes sense to extend `Select-Object`, as \"select every N objects\" seems like a similar operation to \"select the first / last N\" objects.\r\n  * However, from an _implementation_ perspective I can see why packing more into `Select-Object` may be problematic.\r\n  * If a new cmdlet is implemented, I suggest sticking with the `Select` verb, such as `Select-Batch`\r\n\r\n\r\n\r\n",
      "created_at": "2022-09-14T17:01:10Z",
      "updated_at": "2022-09-14T17:01:10Z"
    },
    {
      "author": "dkaszews",
      "author_association": "CONTRIBUTOR",
      "body": "I'll try to cook something up in @iRon7 's repo, I was thinking of naming it `Select-Many` like in C# LINQ, but @mklement0 's `Select-Batch` also makes sense if it allows both flattening and chunking. Or we could just throw a bunch of stuff together and think about cleaning it up once it takes shape.",
      "created_at": "2022-09-14T17:47:05Z",
      "updated_at": "2022-09-14T17:47:05Z"
    }
  ],
  "created_at": "2018-11-14T19:47:19Z",
  "number": 8270,
  "state": "open",
  "title": "Suggestion: Add a chunking (partitioning, batching) mechanism to Select-Object, analogous to Get-Content -ReadCount",
  "updated_at": "2022-09-14T17:47:05Z"
}