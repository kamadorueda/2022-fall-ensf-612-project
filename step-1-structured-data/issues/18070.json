{
  "_url": "https://github.com/PowerShell/PowerShell/issues/18070",
  "author": "iRon7",
  "body": "### Summary of the new feature / enhancement\r\n\r\nAs appears from #8270 [Suggestion: Add a chunking (partitioning, batching) mechanism to Select-Object, analogous to Get-Content -ReadCount](https://github.com/PowerShell/PowerShell/issues/8270#issuecomment-1243457103) the concerned cmdlets run faster when fed with batches. <strike>Presumable because the output file is (re)opened and closed for each individual item (or *batch*)</strike>.\r\nThis suggested that a `-WriteCount` parameter as apposed to the `-ReadCount` parameter in `Get-Content` (which keeps the process/file open for a longer period) might improve the performance of the concerned cmdlets.\r\n\r\n<strike>\r\nIf the assumption\u00b9 is correct that the output file is (re)opened and closed for each item in the pipeline, a simple \"`-KeepOpen`\" switch (or even a fixed change) where the file is closed at the end of the pipeline (output timeout) or the cmdlet is finalized might also a direction to improve the performance of these cmdlets.\r\n\r\n<sup>1) I lack the C# knowledge to reverse engineer the concerned cmdlets and therefore can only base my assumptions on PowerShell knowledge and (performance) testing.</sup>\r\n</strike>\r\n\r\n```Console\r\nName                           Value\r\n----                           -----\r\nPSVersion                      7.2.6\r\nPSEdition                      Core\r\nGitCommitId                    7.2.6\r\nOS                             Microsoft Windows 10.0.22000\r\nPlatform                       Win32NT\r\nPSCompatibleVersions           {1.0, 2.0, 3.0, 4.0\u2026}\r\nPSRemotingProtocolVersion      2.3\r\nSerializationVersion           1.1.0.1\r\nWSManStackVersion              3.0\r\n```\r\n",
  "closed_at": "2022-09-19T00:01:21Z",
  "comments": [
    {
      "author": "jhoneill",
      "author_association": "NONE",
      "body": "@iRon7  that's not what your comment shows, I've answered it there, but your examples don't output the correct CSV.  \r\n\r\nOn my machine \r\n`1..100000 |ForEach-Object { [pscustomobject]@{ id = $_; name = \"name$_\" } }   |Export-Csv .\\test2.csv`   \r\ntakes about 20% longer than    \r\n`1..100000 |ForEach-Object { [pscustomobject]@{ id = $_; name = \"name$_\" } }   | convertto-csv | out-null`\r\n\r\n(the times are 1.2 vs 1.0 secs, ) and \r\n`1..100000 |ForEach-Object { [pscustomobject]@{ id = $_; name = \"name$_\" } }   | out-null`\r\ntakes 0.75 secs.   So the file operations aren't  much of the total.  \r\n\r\nLooking at export-csv it doesn't open and close the file in the `process` block, it uses a stream-writer object and it flushes *that*  as the last line of the process block. The `end` block calls code to flush and dispose of the stream-writer and the file-stream it uses.  **Possibly**  flushing the stream-writer less often would give a small perf boost (a few large writes should be faster than many small ones - I have a fast SSD in this machine, a slower disk would show bigger gains). So I wouldn't rule this out completely, but it may not be the huge improvement you thought. \r\n\r\n\r\n\r\n ",
      "created_at": "2022-09-12T10:19:08Z",
      "updated_at": "2022-09-12T10:19:08Z"
    },
    {
      "author": "IISResetMe",
      "author_association": "COLLABORATOR",
      "body": "> Presumable because the output file is (re)opened and closed for each individual item (or batch).\r\n\r\nThis is _not_ the case - `Set-Content` opens at most 2 file handles per pipeline invocation (1 for BOM detection, 1 for writing new content) - there's no closing/re-opening happening in between pipeline input being processed.\r\n\r\nThe speedup comes (as @jhoneill explains above) from the fact that you only write 0.01% of the data you would have otherwise written to disk :) ",
      "created_at": "2022-09-12T10:43:21Z",
      "updated_at": "2022-09-12T10:43:21Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "@jhoneill , @IISResetMe ,\r\n\r\nThanks for explaining that ***there's no closing/re-opening happening***.\r\n\r\nMy test with regards to the `Export-Csv` cmdlet were indeed incorrect (I removed it from the issue title) but I still see quiet a performance difference for the `Set-Content` cmdlet were the resulting output file is exactly the same:\r\n\r\n```PowerShell\r\n# This takes about 11 seconds:\r\n1..1000000 |Set-Content .\\test1\r\n# This takes about 5 seconds (even there is an addition cmdlet in the chain):\r\n1..1000000 |Create-Batch -Size 10000 |Set-Content .\\test2\r\n# (All in once) takes less than 0.5 seconds:\r\n(,@(1..1000000)) |Set-Content .\\test3\r\n```\r\n\r\n(Independent of the cause -that I might not understand-, it suggests that `Set-Content` could perform faster, or am I still missing something?)",
      "created_at": "2022-09-12T12:08:00Z",
      "updated_at": "2022-09-12T13:23:15Z"
    },
    {
      "author": "jhoneill",
      "author_association": "NONE",
      "body": "The fastest way is\r\n``` \r\n$x = 1..1000000  \r\nSet-Content .\\test1 $x \r\n```\r\n\r\nYou've show that something in the way `Set-Content` processes piped items can be optimized - but it would be very surprising if it went faster than avoiding the pipe completely and passing the items in a single step.. \r\n",
      "created_at": "2022-09-12T14:41:27Z",
      "updated_at": "2022-09-12T14:41:27Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "> but it would be very surprising if it went faster than avoiding the pipe completely\r\n\r\nI do not expect that but in case I do use the pipeline (e.g. to save memory), I actually do expect that it could be about twice as fast.",
      "created_at": "2022-09-12T15:50:09Z",
      "updated_at": "2022-09-12T15:50:09Z"
    },
    {
      "author": "SteveL-MSFT",
      "author_association": "MEMBER",
      "body": "Anything that passes through the pipeline will be slower than than going through the pipeline.  In this case, `Set-Content file $x` bypasses the pipeline completely whereas `$x | Set-content file` will pass each item, one-by-one, which is inherently slower.  Or piping the array/collection at once vs each element: `,$x | Set-content file` will be almost as pass as not using the pipeline at all.",
      "created_at": "2022-09-12T17:57:46Z",
      "updated_at": "2022-09-12T17:57:46Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "@[SteveL-MSFT](https://github.com/SteveL-MSFT),\r\n(Sorry, I have made a wrong and unclear start with this issue)\r\n\r\n> Anything that passes through the pipeline will be slower than than going through the pipeline.\r\n\r\nThat is what I would assume, but what about [this](https://github.com/PowerShell/PowerShell/issues/8270#issuecomment-1243457103):\r\n\r\n```PowerShell\r\nfunction Create-Batch {\r\n    [CmdletBinding()]\r\n    param (\r\n        [ULong]$Size,\r\n        [Parameter(Mandatory=$true, ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true)]$InputObject\r\n    )\r\n    begin {\r\n        $Batch = [Collections.Generic.List[object]]::new()\r\n    }\r\n    process {\r\n        if ($Size -and $Batch.get_Count() -ge $Size) {\r\n            ,$Batch\r\n            $Batch = [Collections.Generic.List[object]]::new()\r\n        }\r\n        $Batch.Add($_)\r\n    }\r\n    End {\r\n        if ($Batch.get_Count()) { ,$Batch }\r\n    }\r\n}\r\n\r\n(Measure-Command {\r\n    1..1000000 |Set-Content .\\test1\r\n}).TotalSeconds\r\n6,8840292\r\n\r\n(Measure-Command {\r\n    1..1000000 |Create-Batch -Size 10000 |Set-Content .\\test2\r\n}).TotalSeconds\r\n2,5975531\r\n```\r\n\r\nIf I take the last part **`|Create-Batch -Size 10000 |Set-Content .\\test2`** and consider it as a single cmdlet, how can it be faster than just **`|Set-Content .\\test1`**?",
      "created_at": "2022-09-13T07:48:49Z",
      "updated_at": "2022-09-13T12:43:01Z"
    },
    {
      "author": "jhoneill",
      "author_association": "NONE",
      "body": "> Anything that passes through the pipeline will be slower than than going through the pipeline. In this case, `Set-Content file $x` bypasses the pipeline completely whereas `$x | Set-content file` will pass each item, one-by-one, which is inherently slower. \r\n\r\n@SteveL-MSFT   what is strange with this specific case is \r\n1. The time that  `$x | Set-content file`  takes scales linearly with the number of items in $x. So there isn't a `$buffer = $buffer + $NewItem` process copying more and more data as $x increases - because _that_ takes order N-Squared time. \r\n2.  `$x | create-batch | Set-content file`  sends more down the pipeline - the same number of items get piped into `Create-batch` which does some work, and sends further items into `set-content` but the result is faster, despite the extra work happening to create and then unroll the array passed as a batch.  So batching is cutting out something slow which runs once per item in `Set-Content` \r\n\r\nI still think the easy answer is if speed is an issue, and the total file size doesn't make memory a limitation, would be just to use\r\n`$temp = Some command  ; set-content -file file -value $temp`  ",
      "created_at": "2022-09-13T11:00:52Z",
      "updated_at": "2022-09-14T12:29:09Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "@SteveL-MSFT,\r\nDo we agree that the odd performance behavior shown here is **not** answered?\r\nIf yes, can you remove the `Resolution-Answered` label to prevent it from automatically being closed?",
      "created_at": "2022-09-14T12:19:55Z",
      "updated_at": "2022-09-14T12:36:57Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "Note that the same behavior can be seen on the `Out-File` cmdlet:\r\n```PowerShell\r\n(Measure-Command {\r\n     1..1000000 |Create-Batch -Size 10000 |Out-File .\\OutFile2.txt\r\n}).TotalSeconds\r\n6,7076176\r\n```\r\nvs.\r\n```PowerShell\r\n(Measure-Command {\r\n    1..1000000 |Out-File .\\OutFile1.txt\r\n}).TotalSeconds\r\n16,6025553\r\n```\r\nI have not been able to exclude the file system (write cache?) any other cmdlets that I have tested so far are slower with batches.\r\n",
      "created_at": "2022-09-14T13:55:15Z",
      "updated_at": "2022-09-14T13:56:09Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "@iRon7, in your example we're dealing with 1 million vs. 1,000 calls to `Set-Content`'s `ProcessRecord()` method.\r\n\r\nWhile method calls always add overhead, I think what makes them particularly expensive in the case of `Set-Content` is the _parameter-binding_ overhead: The pipeline-binding `-Value` parameter is `object[]`-typed, which means that for each _scalar_ input object a _one-element wrapper array_ is created.\r\n\r\nThat is, 1 million 1-element arrays must be created on the fly in the non-batched call.\r\nIn the batched call, 1000 `List<object>` instances must be converted to `object[]`.\r\n\r\nDeclaring the  parameter as a _scalar_ `object`, and letting the cmdlet's implementation determine whether a given input object is an array (enumerable) or not would improve performance.\r\nGiven that non-batched input is the typical use case, this seems worth doing.\r\n\r\nThe following benchmark code compares an `object[]`-typed (`*_ArrayParam()`) parameter with an `object`-typed one (`*_ScalarParam`), with respect to all-at-once, batched, and per-item (element) input.\r\n\r\nNote that it is optimized for `object[]` input, specifically; seemingly, enumerating an `object` found to implement `IEnumerable` _via that interface_ in `foreach` is much slower than iterating over `object[]`\r\n\r\n<details>\r\n<summary>Benchmark code</summary>\r\n\r\n```powershell\r\nAdd-Type -ErrorAction Stop @'\r\nusing System.Collections;\r\npublic class Foo \r\n{\r\n  object[] all = new object[1000000];\r\n  object[] chunks = new object[1000];\r\n\r\n  public Foo()\r\n  {\r\n    for (int i = 0; i < 1000; ++i)\r\n    {\r\n      chunks[i] = new object[1000];\r\n    }\r\n  }\r\n  public void PerItem_ArrayParam()\r\n  {\r\n    foreach (var el in all)\r\n    {\r\n      // Because -Value is object[]-typed, each scalar\r\n      // is wrapped in an array on parameter binding.\r\n      ProcessRecord_ArrayParam((new object[1] { el }));\r\n    }\r\n  }\r\n\r\n  public void PerItem_ScalarParam()\r\n  {\r\n    foreach (var el in all)\r\n    {\r\n      ProcessRecord_ScalarParam(el);\r\n    }\r\n  }\r\n\r\n  public void Batched_ArrayParam()\r\n  {\r\n    for (int i = 0; i < 1000; ++i)\r\n    {\r\n      ProcessRecord_ArrayParam((object[])chunks[i]);\r\n    }\r\n  }\r\n\r\n  public void Batched_ScalarParam()\r\n  {\r\n    for (int i = 0; i < 1000; ++i)\r\n    {\r\n      ProcessRecord_ScalarParam(chunks[i]);\r\n    }\r\n  }\r\n\r\n  public void AllAtOnce_ArrayParam()\r\n  {\r\n    ProcessRecord_ArrayParam(all);\r\n  }\r\n\r\n  public void AllAtOnce_ScalarParam()\r\n  {\r\n    ProcessRecord_ScalarParam(all);\r\n  }\r\n\r\n  // Array parameter, as Set-Content currently uses.\r\n  public void ProcessRecord_ArrayParam(object[] arr)\r\n  {\r\n    foreach (var el in arr)\r\n    {\r\n      var dummy = el;\r\n    }\r\n  }\r\n  \r\n  // Scalar object parameter (which may receive arrays / collections)\r\n  public void ProcessRecord_ScalarParam(object o)\r\n  {\r\n    if (o is object[] oarr) // Optimized for regular PS arrays. `Array` would be much slower, like `IEnumerable` below.\r\n    {\r\n      foreach (var el in oarr)\r\n      {\r\n        var dummy = el;\r\n      }\r\n    }\r\n    else if (o is IEnumerable ienum) // !! This is considerably slower - in practice, more tests are needed to rule out hashtables, ...\r\n    {\r\n      foreach (var el in ienum)\r\n      {\r\n        var dummy = el;\r\n      }\r\n    }\r\n    else  // scalar, use as-is.\r\n    {       \r\n       var dummy = o;\r\n    }\r\n  }\r\n}\r\n'@\r\n\r\n# Download and define function `Time-Command` on demand (will prompt).\r\n# To be safe, inspect the source code at the specified URL first.\r\nif (-not (Get-Command -ErrorAction Ignore Time-Command)) {\r\n  $gistUrl = 'https://gist.github.com/mklement0/9e1f13978620b09ab2d15da5535d1b27/raw/Time-Command.ps1'\r\n  if ((Read-Host \"`n====`n  OK to download and define benchmark function ``Time-Command```n  from Gist ${gistUrl}?`n=====`n(y/n)?\").Trim() -notin 'y', 'yes') { Write-Warning 'Aborted.'; exit 2 }\r\n  Invoke-RestMethod $gistUrl | Invoke-Expression 3>$null\r\n  if (-not ${function:Time-Command}) { exit 2 }\r\n}\r\n\r\n$o = [Foo]::new()\r\nTime-Command @(\r\n  { $o.PerItem_ArrayParam() }, \r\n  { $o.Batched_ArrayParam() }, \r\n  { $o.AllAtOnce_ArrayParam() }, \r\n  { $o.PerItem_ScalarParam() },\r\n  { $o.Batched_ScalarParam() }, \r\n  { $o.AllAtOnce_ScalarParam() }\r\n)\r\n```\r\n\r\n</details>\r\n\r\nSample results (M1 MacBook Pro, PowerShell Core 7.3.0-preview.7:\r\n\r\n```none\r\nFactor Secs (15-run avg.) Command                    TimeSpan\r\n------ ------------------ -------                    --------\r\n1.00   0.001              $o.AllAtOnce_ArrayParam()  00:00:00.0012294\r\n1.01   0.001              $o.Batched_ArrayParam()    00:00:00.0012440\r\n1.02   0.001              $o.AllAtOnce_ScalarParam() 00:00:00.0012580\r\n1.06   0.001              $o.Batched_ScalarParam()   00:00:00.0012976\r\n4.10   0.005              $o.PerItem_ScalarParam()   00:00:00.0050410\r\n7.95   0.010              $o.PerItem_ArrayParam()    00:00:00.0097780\r\n```\r\n\r\nAs you can see, the performance of per-item processing improves roughly by a factor of 2, at least in this simple implementation - in practice, the determination of what what types should be enumerated presumably takes longer.\r\n\r\n\r\n",
      "created_at": "2022-09-14T16:21:06Z",
      "updated_at": "2022-09-14T16:21:06Z"
    },
    {
      "author": "jhoneill",
      "author_association": "NONE",
      "body": "Just for fun \r\n```\r\nPS>  (Measure-Command {\r\n>>     1..1000000 |Out-File .\\OutFile1.txt\r\n>> }).TotalSeconds\r\n18.2000021\r\n\r\nPS>  (Measure-Command {\r\n>>     1..1000000 | out-string -Stream |Out-File .\\OutFile1.txt\r\n>> }).TotalSeconds\r\n30.2555237\r\n\r\nPS>  (Measure-Command {\r\n>>     1..1000000 | out-string  |Out-File .\\OutFile1.txt\r\n>> }).TotalSeconds\r\n11.9004971\r\n\r\nPS>  (Measure-Command {\r\n>>    $x = 1..1000000 ; out-string -InputObject $x  |Out-File .\\OutFile1.txt\r\n>> }).TotalSeconds\r\n6.4771121\r\n\r\n````\r\n\r\n1 is  the \"control\"\r\n2 Shows out-string -stream gets slow with many items. (Strings appending?)\r\n3. Shows we can make Out-File go faster by sending Strings to it instead of numbers. No, I don't know why, but I'm guessing there's a not very efficient format operation.\r\n4. Shows not piping to to Out-String cuts the time more ",
      "created_at": "2022-09-14T16:21:50Z",
      "updated_at": "2022-09-14T16:21:50Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "Note that `Out-File` and `Out-String` already have _scalar_ `-InputObject` parameters, so they don't suffer from the same array-creation-per-scalar-input-object problem that `Set-Content` does.\r\n\r\n(As an aside: they have `psobject` parameters, not `object`; see discussion in https://github.com/PowerShell/PowerShell/issues/5551)\r\n\r\nThus, perhaps the performance differences in your calls are primarily due to the count of `ProcessRecord()` calls involved:\r\n\r\n(1) has 1 million calls for `Out-File`\r\n(2) has 1 million calls for `Out-String` and 1 million for `Out-File`\r\n(3) has 1 million calls for `Out-String`, but only 1 for `Out-File` (because `Out-String` without `-Stream` emits a _single_ string).\r\n(4) has 1 call for `Out-String`, and 1 for `Out-File`\r\n\r\n\r\n",
      "created_at": "2022-09-14T16:39:58Z",
      "updated_at": "2022-09-14T16:39:58Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "@mklement0, @jhoneill,\r\n\r\nThanks for all the investigations.\r\nBtw, I am maintaining an updated [`Create-Batch` cmdlet](https://github.com/iRon7/Create-Batch) which e.g. now returns a `Object[]` (Rather than a `Generic.List`) batches. For this, I changed the output from `,$Batch` to `,@($Batch)` of virtually no additional costs which might indeed indicate something like a delayed execution that happens at the end-cmdlet which is either for each per-item or a whole batch.",
      "created_at": "2022-09-14T17:34:14Z",
      "updated_at": "2022-09-14T17:34:14Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "@iRon7, returning `object[]` makes sense; a small caveat re `@()`: it is apparently optimized to recognize a `List<object>` operand and seemingly then merely calls its `.ToArray()` method, which is very fast.\r\n\r\nGenerally, however, `@()` _enumerates_  an expression value and builds an `object[]` array from the collected results, which is much slower. It would do that with `List<int>` for instance (element type other than `object`), and also does it with `ArrayList` (though it would seem that the same optimization as with `List<object>` could be applied). \r\n\r\nThus, generally speaking, `.ToArray()`, if available on a list type, performs much better (though you won't necessarily get an `object[]` array).",
      "created_at": "2022-09-14T20:12:06Z",
      "updated_at": "2022-09-14T20:12:06Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "@mklement0,\r\n\r\n> While method calls always add overhead\r\n\r\nI guess that this actually means that the purpose [#18071 `ConvertTo-Csv` and `Export-Csv` should unroll (embedded) arrays and lists](https://github.com/PowerShell/PowerShell/issues/18071) will _**not** result in an improved processing speed_ when supplied with all-at-once - or batched objects input but actually to a commonly  lesser performance for usual per-item (element) inputs (and a similar performance as current cmdlet when supplied with all-at-once or batched of objects).",
      "created_at": "2022-09-15T08:24:03Z",
      "updated_at": "2022-09-15T08:24:03Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "@iRon7, I originally neglected to consider that the overhead doesn't just come from method calls, but also from per-pipeline-input-object _parameter binding_ (even when the avoidable overhead of array allocation for scalar inputs is not in the picture).\r\n\r\nHere's my current understanding:\r\n\r\n* Changing an existing cmdlet that does _not_ perform its own enumeration to doing so _will_ slow down streaming one-by-one processing, but, if done right, only _marginally_. This comes from having to perform reflection on each input object to determine if it is an enumerable or not.\r\n\r\n* If done right, making a cmdlet perform its own enumeration _does_ result in significant performance gains with all-at-once input or (large) batches.\r\n\r\nHere's a proof-of-concept, based on defining two simple binary cmdlets, `Add-OneSimple` and `Add-OneEnumerate`, the former _not_ doing its own enumeration, the latter doing so.\r\nThey accept `object` input, to which, if an object is found to be an `int`, `1` is added on output.\r\nIn `Add-OneEnumerate`, ` LanguagePrimitives.GetEnumerator()` is used to conditionally return an enumerator, if an object is found to be enumerable.\r\n\r\n<details>\r\n<summary>Code</summary>\r\n\r\n```\r\nAdd-Type @'\r\n    using System;\r\n    using System.Management.Automation;\r\n    using System.Collections;\r\n\r\n    [Cmdlet(\"Add\", \"OneSimple\")]\r\n    public class AddOneCommandSimple : PSCmdlet {\r\n\r\n        [Parameter(ValueFromPipeline=true, Position=0)]\r\n        public object InputObject { get; set; }\r\n\r\n        // Some dummy parameters\r\n        [Parameter()]\r\n        public string Dummy1 { get; set; }\r\n        [Parameter()]\r\n        public int Dummy2 { get; set; }\r\n        [Parameter()]\r\n        public long Dummy3 { get; set; }\r\n        [Parameter()]\r\n        public string Dummy4 { get; set; }\r\n\r\n        protected override void ProcessRecord() {\r\n            object o = InputObject is PSObject pso ? pso.BaseObject : InputObject;\r\n            if (o is int i)\r\n            {\r\n                WriteObject(i + 1);\r\n            }\r\n            else\r\n            {\r\n                WriteObject($@\"not an int: {o}, {o.GetType().Name}\");\r\n            }\r\n        }\r\n    }\r\n\r\n    [Cmdlet(\"Add\", \"OneEnumerate\")]\r\n    public class AddOneCommandEnumerate : PSCmdlet {\r\n\r\n        [Parameter(ValueFromPipeline=true, Position=0)]\r\n        public object InputObject { get; set; }\r\n\r\n        // Some dummy parameters\r\n        [Parameter()]\r\n        public string Dummy1 { get; set; }\r\n        [Parameter()]\r\n        public int Dummy2 { get; set; }\r\n        [Parameter()]\r\n        public long Dummy3 { get; set; }\r\n        [Parameter()]\r\n        public string Dummy4 { get; set; }\r\n\r\n        protected override void ProcessRecord() {\r\n            IEnumerator enumerator = LanguagePrimitives.GetEnumerator(InputObject);\r\n            if (enumerator is not null)\r\n            {\r\n                while (enumerator.MoveNext())\r\n                {\r\n                    Impl(enumerator.Current);\r\n                }\r\n            }\r\n            else {\r\n                Impl(InputObject);\r\n            }\r\n        }\r\n\r\n        private void Impl(object o) {\r\n            if (o is PSObject pso)\r\n            {\r\n                o = pso.BaseObject;\r\n            }\r\n            if (o is int i)\r\n            {\r\n                WriteObject(i + 1);\r\n            }\r\n            else\r\n            {\r\n                WriteObject($@\"not an int: {o}, {o.GetType().Name}\");\r\n            }\r\n        }\r\n    }\r\n\r\n'@ -PassThru | ForEach-Object Assembly | Import-Module\r\n\r\n# Download and define function `Time-Command` on demand (will prompt).\r\n# To be safe, inspect the source code at the specified URL first.\r\nif (-not (Get-Command -ErrorAction Ignore Time-Command)) {\r\n    $gistUrl = 'https://gist.github.com/mklement0/9e1f13978620b09ab2d15da5535d1b27/raw/Time-Command.ps1'\r\n    if ((Read-Host \"`n====`n  OK to download and define benchmark function ``Time-Command```n  from Gist ${gistUrl}?`n=====`n(y/n)?\").Trim() -notin 'y', 'yes') { Write-Warning 'Aborted.'; exit 2 }\r\n    Invoke-RestMethod $gistUrl | Invoke-Expression 3>$null\r\n    if (-not ${function:Time-Command}) { exit 2 }\r\n  }\r\n\r\n# Sample input array ([object[]])\r\n$a = 1..1e5\r\n\r\nTime-Command @(\r\n  { $null = $a | Add-OneSimple },\r\n  { $null = $a | Add-OneEnumerate },\r\n  { $null = , $a | Add-OneEnumerate },\r\n  { $null = Add-OneEnumerate -InputObject $a }\r\n)\r\n```\r\n</details>\r\n\r\nSample timings with `100,000` input objects, from my M1 MacBook Pro running PowerShell Core 7.3.0-preview.7:\r\n\r\n```none\r\nFactor Secs (15-run avg.) Command                                  TimeSpan\r\n------ ------------------ -------                                  --------\r\n1.00   0.033              $null = , $a | Add-OneEnumerate          00:00:00.0326625\r\n1.01   0.033              $null = Add-OneEnumerate -InputObject $a 00:00:00.0329941\r\n7.46   0.244              $null = $a | Add-OneSimple               00:00:00.2437299\r\n7.64   0.250              $null = $a | Add-OneEnumerate            00:00:00.2496487\r\n```\r\n\r\nAs you can see:\r\n* the all-input-at-once calls significantly outperform the one-by-one calls\r\n* in the one-by-one calls, the enumerating cmdlet is only marginally slower than the non-enumerating one\r\n\r\n",
      "created_at": "2022-09-15T19:24:12Z",
      "updated_at": "2022-09-16T13:15:08Z"
    },
    {
      "author": "iRon7",
      "author_association": "NONE",
      "body": "@mklement0, thanks again for the further investigations and clarifications \r\n\r\n>  ... but, if done right, only marginally ...\r\n\r\nBottom line, I understand that this all means:\r\n1. that besides that there wouldn't virtually any performance disadvantage of implementing an all-at-once/\"batch\" input for cmdlets along with `ConvertTo-Csv`/`ExportTo-Csv` (#18071),\r\n2. the cmdlets `Set-Content`/`Add-Content` and `Out-File` (and all other cmdlets that support an all-at-once/\"batch\" input including the default display output/`Write-Output`) might actually _not_ do the right thing here?\r\n\r\n\r\n",
      "created_at": "2022-09-17T11:21:00Z",
      "updated_at": "2022-09-17T11:21:00Z"
    },
    {
      "author": "mklement0",
      "author_association": "CONTRIBUTOR",
      "body": "Re 1: Yes,  I think that `ConvertTo-Csv` and `Export-Csv` - along with all the other ones in Category C (from #4242) - could be \"batch-enhanced\" without negatively impacting one-by-one processing (in a meaningful way).\r\n\r\nRe 2: Among the Category B cmdlets - which are \"batch-enhanced\" already - `Set-Content` / `Add-Content`  and `Join-Content` could be improved to benefit the _one-by-one_ case, by defining their `-Value` / `-InputObject` as `object` rather than `object[]`, which the others in that category, including `Out-File`, already do.\r\n",
      "created_at": "2022-09-17T13:21:18Z",
      "updated_at": "2022-09-17T13:21:46Z"
    },
    {
      "author": "msftbot[bot]",
      "author_association": "NONE",
      "body": "This issue has been marked as answered and has not had any activity for **1 day**. It has been closed for housekeeping purposes.",
      "created_at": "2022-09-19T00:01:20Z",
      "updated_at": "2022-09-19T00:01:20Z"
    }
  ],
  "created_at": "2022-09-12T09:25:35Z",
  "number": 18070,
  "state": "closed",
  "title": "Possible `Set/Add-Content` performance improvement",
  "updated_at": "2022-09-19T00:01:21Z"
}